\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts,amsmath,amssymb,suetterl}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[utf8]{inputenc}

\usepackage{fontawesome}
\DeclareUnicodeCharacter{2212}{-}
\usepackage{mathrsfs}
\usepackage{hyperref}

\usepackage[nodisplayskipstretch]{setspace}

\setstretch{1.5}

\fancyfoot[C]{\thepage}

\renewcommand{\footrulewidth}{0pt}
\parindent 0ex
\setlength{\parskip}{1em}

\newenvironment{psmallmatrix}
  {\left(\begin{smallmatrix}}
  {\end{smallmatrix}\right)}

\begin{document}
    \section*{MTH6141 Random Processes, 2013â€”14\\Solutions to Exercise Sheet 5}
    \begin{enumerate}
        \item As in the lecture, we argue that, in order to be at, state $0$ at time $2n$, we must have made exactly $n$ increasing transitions and $n$ decreasing. There are
        $
        \begin{psmallmatrix}
            2n \\
            n
        \end{psmallmatrix}
        $
        possible such sequences. Each sequence occurs with probability $p^n(1-p)^n$. Thus the probability of making a transition from state 0 to state 0 in $2n$ steps is
        $
        \begin{psmallmatrix}
            2n\\
            n
        \end{psmallmatrix}
        p^n(1-p)^n
        $.
        It follows that
        $$
        p_{00}^{(2n)}
        =
        \begin{pmatrix}
            2n\\
            n
        \end{pmatrix}
        p^n(1-p)^n \leq 2^{2n}[p(1-p)]^n
        =
        [4p(1-p)]^n,
        $$
        where we have used the fact that the binomial coefficient 
        $
        \begin{psmallmatrix}
            2n\\
            n
        \end{psmallmatrix}
        $
        is certainly less than the sum 
        $
        \sum_{k=0}^{2n}
        \begin{psmallmatrix}
            2n\\
            k
        \end{psmallmatrix}
        =
        2^{2n}
        $
        of all coefficients. The function $r = r(p) = 4p(1-p)$ achieves a maximum of $1$ at the point $p = \frac{1}{2}$ and is everywhere else strictly less than $1$ (and non-negative). So when $p \neq \frac{1}{2},\ r<1$ and
        $$
        \sum_{t=1}^\infty p_{00}^{(t)} = \sum_{n=1}^\infty p_{00}^{(2n)} \leq \sum_{n=1}^\infty r^n \leq \frac{1}{1-r} < \infty,
        $$
        where we have used the fact that $p_{00}^{(t)} = 0$ when $t$ is odd. Hence, by Lemma 1.13, state 0 is transient. By symmetry (or by Theorem 1.14) all states arc transient.
        %
        \item Consider the symmetric random walk $(Y_t)$ on $\mathbb{Z}$ and the related stochastic process $(X_t)$ on $\mathbb{N}$ given by $X_t = |Y_t|$. The transition probabilities of $(X_t)$ are given By
        $$
        p_{ij}
        =
        \mathbb{P}(X_{t+1} = j\, | \, X_t = i)
        =
        \begin{cases}
            \frac{1}{2}, & \text{if}\ i \neq 0\ \text{and}\ |j-i| = 1;\\
            1, & \text{if}\ i = 0\ \text{and}\ j = 1;\ \text{and}\\
            0, & \text{otherwise}.
        \end{cases}
        $$
        These transition probabilities do not depend on the past history $(X_0, X_1, \ldots, X_{t-1})$, so the process is a Markov chain. Note that the transition probabilities precisely agree with those of the random walk on $\mathbb{N}$ with reflecting barrier at $0$. So we can think of the random walk with a on $\mathbb{N}$ with a reflecting barrier at $O$ as being the image of the symmetric random walk on $\mathbb{Z}$ under the map $i \to |i|$. Since $0$ is a recurrent state for the symmetric random walk on $\mathbb{Z}$, and $|X_t| = 0$ iff $Y_t = 0$, the state $0$ is recurrent also for the walk on $\mathbb{N}$ with a reflecting barrier.\par 
        States $999$ and $0$ are in thc same communicating class, so $999$ is recurrent also, by Theorem 1.14.
        %
        \item If we start in state 1 we can return 2 steps $(1 \to 2 \to 1)$ or 3 steps $(1 \to 2 \to 3 \to 1)$. These are the only possibilities, so. So $\mathbb{P}(R_1 = 2)=\frac{2}{3},\ \mathbb{P}(R_1 = 3)=\frac{1}{3}$ and $\mathbb{E}(R_1) = \frac{2}{3}\cdot 2+\frac{1}{3}\cdot 3 = \frac{7}{3}$.\\
        If we start in state 2 then possibile paths are similar $(2 \to 1 \to 2\ \text{and}\ 2 \to 3 \to 1 \to 2)$ and the calculation is identical, yielding $\mathbb{P}(R_2 = 2) = \frac{2}{3},\ \mathbb{P}(R_2=3) \frac{1}{3}$ and $\mathbb{E}(R_2) = \frac{2}{3}\cdot 2 + \frac{1}{3}\cdot 3 = \frac{7}{3}$.\\
        If we start in state 3 then we we may return at any odd number $t = 2k+1$ of steps $(k \geq 1)$, via a path of the form $3 \to 1 \to 2 \to \ldots \to 1 \to 2 \to 3$. Thus $\mathbb{P}(R_3 = 2k+1) = \frac{1}{3}\left(\frac{2}{3}\right)^{k-1}$ for $k \geq 1$ (and all other values for $R_3$ have zero probability). I.e, $\frac{1}{2}(R_3-1)$ is distributed geometrically with parameter $1/3$. Hence $\mathbb{E}(\frac{1}{2}(R_3-1)) = \frac{1}{2}\mathbb{E}(R_3) - \frac{1}{2} = 3$, and $\mathbb{E}(R_3) = 7$. (Or sum the series by hand.)\\
        By Theorem 1.16, the equilibrium distribution is given by $w_k = 1/\mathbb{E}(R_k)$, so is $\left(\frac{3}{7}, \frac{3}{7}, \frac{1}{7}\right)$. This agrees with the answer obtained by solving $wP = w$ (c.f. Exercise Sheet 3).
        \item Suppose that $a \leftrightarrow b$. Then there exist $r$ and $s$ such that $p_{ab}^(r)>0$ and $p_{ab}^{(s)}>0$. Hence
        $$
        p_{aa}^{(r+t+s)} \geq p_{ab}^{(r)}p_{bb}^{(t)}p_{ba}^{(s)} = \alpha p_{bb}^{(t)},\quad
        \text{for all}\ t \in \mathbb{N}
        $$
        where $\alpha = p_{ab}^{(r)}p_{ba}^{(s)}>0$.\\
        Now suppose that $a$ is null recurrent. Recurrence is a class property (Theorem 1.14) and so state $b$ is also recurrent. Now, by Theorem 1.15, since $a$ is null recurrent, $p_{aa}^{(t)} \to 0$ as $t \to \infty$. Now $p_{bb}^{(t)} \leq \alpha^{-1}p_{aa}^{(r+t+s)},$ and the right-hand side converges to $0$ as $t\to \infty$. Hence the left-hand side also converges to $0$ and, by Theorem 1.15, state $b$ is null recurrent.
        %
        \item By considering the transition diagram we see that
        \begin{align*}
            f_{00}^{(t)}
            &= p_{0,1}p_{1,2}p_{2,3}\ldots p_{t-2,t-1}p_{t-1,0}\\
            &= \left(\frac{1}{2}\right)\left(\frac{2}{3}\right)\left(\frac{3}{4}\right)\ldots \left(\frac{t-1}{t}\right)\left(\frac{t}{t+1}\right)\\
            &= \frac{1}{t(t+1)},
        \end{align*}
        since the only sequence of $t$ transitions leading from state 0 back to state 0 (without visiting state 0 betweentimes) is $0 \to 1 \to 2 \to \ldots \to t-2 \to t-1 \to 0$. Now
        $$
        f_{00} = \sum_{t=1}^\infty f_{00}^{(t)} = \sum_{t =1}^\infty \frac{1}{t(t+1)} = \sum_{t=1}^\infty \left(\frac{1}{t}-\frac{1}{t+1}\right) = 1,
        $$
        since all terms, aside from the first, cancel in pairs. As $f_{00} = 1$, the chain is recurrent.\\
        Also,
        $$
        \mathbb{E}(R_0) = \sum_{t = 1}^\infty tf_{00}^{(t)} = \sum_{t=1}^\infty \frac{1}{t+1} = \infty
        $$
        and so the chain is null recurrent.\\
        \textit{Remark.} The transition diagram for this process has the same shape as that for the success runs chain. Only the numerical values of the positive probabiliies have changed. You could think of this process as being like the success runs chain except that the longer the current run of successes is the more likely it is to be extended. For instance a football team may be more confident during a long run of wins and hence more likely to win the next game.
        %
        \item The equation for $w = (w_0, w_1, w_2, \ldots)$ are as follows:
        \begin{align*}
            \frac{1}{2}w_0 &= \frac{1}{3}w_1+\frac{1}{4}w_2+\frac{1}{5}w_3+\ldots\\
            w_1 &= \frac{1}{2}w_0\\
            w_2 &= \frac{2}{3}w_1\\
            &\vdots\\
            w_j &= \frac{j}{j+1}w_{j-1}
        \end{align*}
        Solving for $w_1, w_2,\ldots$ in terms of $w_0$:
        \begin{align*}
            w_1 &= \frac{1}{2}w_0\\
            w_2 &= \frac{2}{3} \times \frac{1}{2} = \frac{1}{3}\\
            w_3 &= \frac{3}{4} \times \frac{1}{3} = \frac{1}{4}\\
            & \vdots \\
            w_j &= \frac{j}{j+1}\times \frac{1}{j} = \frac{1}{j+1}.
        \end{align*}
        Now for $w$ to be a probability distribution we require
        $$
        \sum_{j=0}^\infty w_j = \sum_{j=0}^\infty \frac{w_0}{j+1} = 1.
        $$
        But this impossible, since either $w_0 = 0$, in which case the sum is 0, or $w_0>0$ in which case the series diverges.\\
        So the Markov chain has no equilibrium distribution, and by Theorem 1.16 is null recurrent.
        %
        \item Conditioning on $X_{t-1} = k$ and using the Law of Total Probability,
        \begin{align*}
            \mathbb{P}(X_t = 0\, | \, X_0 = 0)
            &= \sum_{k=0}^\infty \mathbb{P}(X_{t-1}=k\, | \, X_0=0)\mathbb{P}(X_t=0\, | \, X_{t-1}=k)\\
            &= \sum_{k=0}^{t-1} \mathbb{P}(X_{t-1}=k\, | \, X_0=0)\mathbb{P}(X_t=0\, | \, X_{t-1}=k)\\
            &= \sum_{k=0}^{t-1} \mathbb{P}(X_{t-1}=k\, | \, X_0=0)\frac{1}{k+2}\\
            & \geq \sum_{k=0}^{t-1} \mathbb{P}(X_{t-1}=k\, | \, X_0=0)\frac{1}{k+1}\\
            &= \frac{1}{t+1}
        \end{align*}
        In the second line, we have used the used the fact that $X_t \leq t$ for all $t$ (since $X_t$ increases by at most one at each time step).\\
        Since $\sum_{t=1}^\infty p_{00}^{(t)} \geq \sum_{t=1}^\infty \frac{1}{t+1}$ diverges, the Markov chain is recurrent, by Lemma 1.13.
    \end{enumerate}

\end{document}